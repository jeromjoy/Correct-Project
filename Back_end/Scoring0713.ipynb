{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import dateutil.parser as parser\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from urllib.parse import urlparse\n",
    "import re\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoring (news):\n",
    "    article = news.Article\n",
    "    createdDate = news.CreatedDate\n",
    "    title = news.Title\n",
    "    author = news.Author\n",
    "    originalContent = news.OriginalContent\n",
    "    lastUsed = news.LastUsed\n",
    "    href = news.ArticleUrl\n",
    "    fetchedDate = news.FetchedDate\n",
    "    \n",
    "    print (href)\n",
    "    \n",
    "    score = 0\n",
    "    \n",
    "    if (title):\n",
    "        score += 1\n",
    "        \n",
    "    if (article):\n",
    "        list_number = []\n",
    "        score_para = 0\n",
    "        for paragraphs in article:\n",
    "            length = len(re.findall(r'\\w+', paragraphs))\n",
    "            list_number.append(length)\n",
    "        max_number = max(list_number)\n",
    "#         print (max_number)\n",
    "        if (max_number <= 100):\n",
    "            score += 1\n",
    "    \n",
    "    if (createdDate):\n",
    "        createdDate_str = datetime.datetime.strptime(createdDate, \"%Y-%m-%dT%H:%M:%S\")\n",
    "        fetchedDate_str = datetime.datetime.strptime(fetchedDate, \"%Y-%m-%dT%H:%M:%S\")    \n",
    "        duration = fetchedDate_str - createdDate_str\n",
    "         if (0 < duration.days <= 90):\n",
    "                score += 5\n",
    "            elif (91 <= duration.days <= 182):\n",
    "                score += 4\n",
    "            elif (183 <= duration.days <= 273):\n",
    "                score += 3\n",
    "            elif (274 <= duration.days <= 365):\n",
    "                score += 2\n",
    "            else:\n",
    "                score += 1\n",
    "        \n",
    "    if (author):\n",
    "        score += 1\n",
    "    \n",
    "    if (lastUsed):\n",
    "        score += 1\n",
    "    \n",
    "    if (fetchedDate):\n",
    "        score += 1\n",
    "        \n",
    "    print ('********total:**********', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.thedailysheeple.com/our-political-parties-are-obsolete_072017\n",
      "********total:********** 5\n",
      "http://www.thedailysheeple.com/palm-beach-sheriffs-deputy-on-paid-leave-after-child-porn-charges_072017\n",
      "********total:********** 6\n",
      "http://www.thedailysheeple.com/recordings-reveal-fbi-gave-man-a-rifle-urged-him-to-carry-out-mass-shooting-to-defend-islam_072017\n",
      "********total:********** 5\n",
      "http://www.futuremoneytrends.com/ag\n",
      "********total:********** 5\n",
      "http://www.thedailysheeple.com/kurdish-militia-commander-clashes-with-turkey-may-come-within-days_072017\n",
      "********total:********** 6\n",
      "http://www.thedailysheeple.com/study-finds-temperature-adjustments-account-for-nearly-all-of-the-warming-in-climate-data_072017\n",
      "********total:********** 6\n",
      "http://www.thedailysheeple.com/exorcism-business-is-booming-could-cern-be-to-blame-video_072017\n",
      "********total:********** 6\n",
      "http://www.thedailysheeple.com/fbi-allowed-to-hide-details-of-secret-911-report-judge_072017\n",
      "********total:********** 5\n",
      "http://www.thedailysheeple.com/scientists-are-now-issuing-dire-warnings-about-sex-robots_072017\n",
      "********total:********** 5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class News:\n",
    "    Article = []\n",
    "    Title = ''\n",
    "    Author = ''\n",
    "    OriginalContent = ''\n",
    "    CreatedDate = ''\n",
    "    FetchedDate = ''\n",
    "    ArticleUrl = ''\n",
    "    LastUsed = ''\n",
    "    \n",
    "    def __init__(self, article , title, author, originalcontent, createddate, fetcheddate, articleurl, lastused):\n",
    "        self.Article = article\n",
    "        self.Title = title\n",
    "        self.OriginalContent = originalcontent\n",
    "        self.CreatedDate = createddate\n",
    "        self.FetchedDate = fetcheddate\n",
    "        self.ArticleUrl = articleurl\n",
    "        self.LastUsed = lastused\n",
    "        self.Author = author\n",
    "\n",
    "\n",
    "def get_news_firstpage(link):\n",
    "    for i in range(len(link)):\n",
    "        link[i] = 'http://' + link[i]\n",
    "        headers = {'user-agent' : 'Mozilla/5.0'}            \n",
    "        try:\n",
    "            source = requests.get(link[i], headers = headers)\n",
    "            soup = BeautifulSoup(source.content, \"lxml\")\n",
    "        except requests.exceptions.ConnectionError:\n",
    "            print ('Connection refused!!!')\n",
    "        \n",
    "        \n",
    "        if (soup.findAll('article')):\n",
    "            for article in soup.findAll('article'):\n",
    "                if (article.find('a')):\n",
    "                    href = article.find('a').get('href')\n",
    "                    if href.startswith('http'):\n",
    "                        href = href\n",
    "                    else:\n",
    "                        href = link[i] + href\n",
    "                    get_news_content(href)\n",
    "                else:\n",
    "                    print ('not able to get <a>!!')\n",
    "        else: \n",
    "            print ('not able to get <article>!!')\n",
    "        \n",
    "    \n",
    " \n",
    "def get_news_content(href):\n",
    "    headers = {'user-agent' : 'Mozilla/5.0'}\n",
    "    try:\n",
    "        source = requests.get(href, headers = headers)\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        source.status_code = \"Connection refused\"\n",
    "    if (BeautifulSoup(source.content, \"lxml\").body):\n",
    "        soup = BeautifulSoup(source.content, \"lxml\").body\n",
    "        if (soup.findAll('p')):            \n",
    "            article = []\n",
    "            for paragraph in soup.findAll('p'):\n",
    "                article.append(paragraph.text)\n",
    "        else:\n",
    "            article = 'Unknown'\n",
    "        date = time.strftime('%Y-%m-%dT%H:%M:%S',time.localtime(time.time()))\n",
    "        fetchedDate = date\n",
    "        lastUsed = date        \n",
    "        orginalContent = source.content\n",
    "        \n",
    "        \n",
    "        if (urlparse(href).netloc == 'americannews.com'):              \n",
    "            if (soup.findAll('h1')):\n",
    "                for title in soup.findAll('h1'):\n",
    "                    title = title.text\n",
    "            else:\n",
    "                title = 'Unknown'        \n",
    "            if (soup.find('time',{'class': 'rpwe-time published'})):\n",
    "                createdDate = soup.find('time',{'class': 'rpwe-time published'}).text\n",
    "                createdDate = (parser.parse(createdDate)).isoformat()\n",
    "            else: \n",
    "                createdDate = None  \n",
    "\n",
    "            if (soup.findAll('b')):\n",
    "                for author in soup.findAll('b'):\n",
    "                    author = author.text\n",
    "            else:\n",
    "                author = 'Unknown'\n",
    "        \n",
    "        elif (urlparse(href).netloc == 'www.activistpost.com'):\n",
    "            if (soup.findAll('span', {'class': 'entry-meta-date updated'})):\n",
    "                for createdDate in soup.findAll('span', {'class': 'entry-meta-date updated'}):\n",
    "                    createdDate = (parser.parse(createdDate.text)).isoformat()\n",
    "            else:\n",
    "                createdDate = None\n",
    "\n",
    "            if (soup.findAll('h1', {'class': 'entry-title'})):\n",
    "                for title in soup.findAll('h1', {'class': 'entry-title'}):\n",
    "                    title = title.text\n",
    "            else:\n",
    "                title = 'Unknown'\n",
    "            \n",
    "            for authors in soup.findAll('p'):\n",
    "                if (authors.findAll('a')):\n",
    "                    for author in authors.findAll('a'):\n",
    "                        author = author.text\n",
    "                else: \n",
    "                    author = 'Unknown'\n",
    "        \n",
    "        \n",
    "        \n",
    "        elif (urlparse(href).netloc == 'www.thedailysheeple.com'):\n",
    "            if (soup.findAll('time', {'class': 'entry-date'})):\n",
    "                for createdDate in soup.findAll('time', {'class': 'entry-date'}):\n",
    "                    createdDate = (parser.parse(createdDate.text)).isoformat()\n",
    "            else:\n",
    "                createdDate = None\n",
    "\n",
    "                \n",
    "            if (soup.findAll('h1', {'class': 'entry-title'})):\n",
    "                for title in soup.findAll('h1', {'class': 'entry-title'}):\n",
    "                    title = title.text\n",
    "            else:\n",
    "                title = 'Unknown'\n",
    "            \n",
    "            if (soup.findAll('span', {'class': 'author vcard'})):\n",
    "                for author in soup.findAll('span', {'class': 'author vcard'}):\n",
    "                    author = author.text\n",
    "            else: \n",
    "                author = 'Unknown'\n",
    "            \n",
    "        \n",
    "        \n",
    "        elif (urlparse(href).netloc == 'waterfordwhispersnews.com'):\n",
    "            if (soup.findAll('p', {'class': 'byline byline-left '})):\n",
    "                for createdDate in soup.findAll('p', {'class': 'byline byline-left '}):\n",
    "                    # createdDate = (parser.parse(createdDate.text)).isoformat()\n",
    "                    createdDate = None\n",
    "            else:\n",
    "                createdDate = None\n",
    "                \n",
    "            if (soup.findAll('h1', {'class': 'entry-title'})):\n",
    "                for title in soup.findAll('h1', {'class': 'entry-title'}):\n",
    "                    title = title.text\n",
    "            else:\n",
    "                title = 'Unknown'\n",
    "            \n",
    "            if (soup.findAll('span', {'class': 'author vcard'})):\n",
    "                for author in soup.findAll('span', {'class': 'author vcard'}):\n",
    "                    author = author.text\n",
    "            else: \n",
    "                author = 'Unknown'\n",
    "\n",
    "        \n",
    "        \n",
    "        elif (urlparse(href).netloc == 'www.clickhole.com'):\n",
    "\n",
    "            if (soup.findAll('div', {'class': 'pub_date'})):\n",
    "                for createdDate in soup.findAll('div', {'class': 'pub_date'}):\n",
    "                    createdDate = (parser.parse(createdDate.text)).isoformat()\n",
    "            else:\n",
    "                createdDate = None\n",
    "\n",
    "            if (soup.findAll('h1', {'class': 'headline'})):\n",
    "                for title in soup.findAll('h1', {'class': 'headline'}):\n",
    "                    title = title.text\n",
    "            else:\n",
    "                title = 'Unknown'\n",
    "\n",
    "            if (soup.findAll('span', {'class': 'author vcard'})):\n",
    "                for author in soup.findAll('span', {'class': 'author vcard'}):\n",
    "                    author = author.text\n",
    "            else: \n",
    "                author = 'Unknown'\n",
    "\n",
    "        \n",
    "        elif (urlparse(href).netloc == 'theonion.com'):\n",
    "            if (soup.findAll('span', {'class': 'content-published-mobile'})):\n",
    "                for createdDate in soup.findAll('span', {'class': 'content-published-mobile'}):\n",
    "                    createdDate = (parser.parse(createdDate.text)).isoformat()\n",
    "            else:\n",
    "                createdDate = None\n",
    "            \n",
    "            if (soup.findAll('header', {'class': 'content-header'})):\n",
    "                for title in soup.findAll('header', {'class': 'content-header'}):\n",
    "                    title = title.text\n",
    "            else:\n",
    "                title = 'Unknown'\n",
    "            \n",
    "            if (soup.findAll('span', {'class': 'author vcard'})):\n",
    "                for author in soup.findAll('span', {'class': 'author vcard'}):\n",
    "                    author = author.text\n",
    "            else: \n",
    "                author = 'Unknown'\n",
    "                        \n",
    "        else:\n",
    "            title = 'Unknown'\n",
    "            author = 'Unknown'\n",
    "            createdDate = None\n",
    "    else:\n",
    "        print ('can not get connent')\n",
    "        return\n",
    "    news = News(article, title, author, orginalContent, createdDate, date, href, date)\n",
    "    scoring(news)\n",
    "    \n",
    "link = [\n",
    "        'thedailysheeple.com',\n",
    "#         'waterfordwhispersnews.com',\n",
    "#         'clickhole.com',\n",
    "#         'americannews.com',\n",
    "#         'activistpost.com',\n",
    "#         'theonion.com'\n",
    "        ]\n",
    "\n",
    "get_news_firstpage(link)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
